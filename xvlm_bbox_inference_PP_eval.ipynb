{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaaaa438",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import datetime\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import ruamel.yaml as yaml\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.distributed as dist\n",
    "from torchvision import transforms\n",
    "import torchvision.utils as vutils\n",
    "from PIL import Image\n",
    "from torchvision.io import read_image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import utils\n",
    "from dataset import create_dataset, create_sampler, create_loader\n",
    "from dataset.utils import collect_tensor_result, grounding_eval_bbox, grounding_eval_bbox_vlue\n",
    "from models.model_bbox import XVLM\n",
    "from models.tokenization_bert import BertTokenizer\n",
    "from models.tokenization_roberta import RobertaTokenizer\n",
    "from optim import create_optimizer\n",
    "from refTools.refer_python3 import REFER\n",
    "from scheduler import create_scheduler\n",
    "from utils.hdfs_io import hmkdir, hcopy, hexists\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd46f7ea",
   "metadata": {},
   "source": [
    "## Define BBox Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2356464",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xvlm_get_bbox(image, prompts, model, tokenizer, config):\n",
    "    # Evaluate the model on the image & prompts\n",
    "    results = torch.empty((len(prompts),4))\n",
    "    model.eval()\n",
    "\n",
    "    for i, text in enumerate(prompts):\n",
    "\n",
    "        image = image.to(device)\n",
    "        text_input = tokenizer(text, padding='longest', return_tensors=\"pt\").to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs_coord = model(image, text_input.input_ids, text_input.attention_mask, target_bbox=None)\n",
    "\n",
    "        results[i] = outputs_coord.cpu()\n",
    "\n",
    "    # convert predicted coordinates from normalized center coordinates to xmin, ymin, xmax, ymax\n",
    "    cx, cy, w, h = results[:,0], results[:,1], results[:,2], results[:,3]\n",
    "    coords = torch.stack((cx - w/2, cy-h/2, cx + w/2, cy + h/2)).T * config['image_res']\n",
    "    \n",
    "    return coords\n",
    "    \n",
    "    \n",
    "    \n",
    "def plot_bbox_prompts(image_int, coords, prompts):\n",
    "    # Plot the resulting predictions on top of the original image\n",
    "    plt.figure(figsize=(10,20))\n",
    "    bbox_image = vutils.draw_bounding_boxes(image_int,coords, \n",
    "                                            width=3, \n",
    "                                            labels=[f\"Prompt {i}\" for i in range(len(prompts))],\n",
    "                                            colors=\"red\")\n",
    "\n",
    "    plt.imshow(bbox_image.permute(1,2,0))\n",
    "    plt.xlabel('\\n' + '\\n'.join([f'Prompt {i}: ' + prompts[i] for i in range(len(prompts))]), fontsize=14)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def plot_competing_bbox_prompts(image_int, coords_1, coords_2, prompts):\n",
    "    # Plot the resulting predictions on top of the original image\n",
    "    plt.figure(figsize=(10,20))\n",
    "    bbox_image = vutils.draw_bounding_boxes(image_int,coords_1, \n",
    "                                            width=3, \n",
    "                                            labels=[f\"Prompt {i}\" for i in range(len(prompts))],\n",
    "                                            colors=\"red\")\n",
    "    \n",
    "    bbox_image_2 = vutils.draw_bounding_boxes(bbox_image,coords_2, \n",
    "                                            width=3, \n",
    "                                            labels=[f\"Prompt {i}\" for i in range(len(prompts))],\n",
    "                                            colors=\"blue\")\n",
    "\n",
    "    plt.imshow(bbox_image_2.permute(1,2,0))\n",
    "    plt.xlabel('\\n' + '\\n'.join([f'Prompt {i}: ' + prompts[i] for i in range(len(prompts))]), fontsize=14)\n",
    "#     plt.legend([],[])\n",
    "    plt.title(\"Red=Object XVLM, Blue=Part XVLM\")\n",
    "    plt.show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c141aea",
   "metadata": {},
   "source": [
    "## Define Parameters & Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd7ea27",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = \"configs/Grounding_bbox.yaml\"\n",
    "config = yaml.load(open(config_file, 'r'), Loader=yaml.Loader)\n",
    "checkpoint = \"model_checkpoints/16m_base_finetune/refcoco_bbox/checkpoint_best.pth\"\n",
    "\n",
    "\n",
    "config_file_pp = \"configs/Grounding_bbox_PascalParts.yaml\"\n",
    "config_pp = yaml.load(open(config_file_pp, 'r'), Loader=yaml.Loader)\n",
    "# checkpoint_pp = \"output/grounding_s_0.1_pascalparts/checkpoint_best.pth\"\n",
    "# checkpoint_pp = \"output/grounding_scaled_lr_pascalparts/checkpoint_best.pth\"\n",
    "checkpoint_pp = \"output/grounding_pascalparts/checkpoint_best.pth\"\n",
    "\n",
    "evaluate = False\n",
    "device = 7\n",
    "\n",
    "\n",
    "# Transformations\n",
    "normalize = transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "        transforms.Resize((config['image_res'], config['image_res']), interpolation=Image.BICUBIC),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ])\n",
    "\n",
    "plot_transform = transforms.Compose([\n",
    "    transforms.Resize((config['image_res'], config['image_res']), interpolation=Image.BICUBIC),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.ConvertImageDtype(torch.uint8),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377b3490",
   "metadata": {},
   "source": [
    "## Load and Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390750ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating model\")\n",
    "model = XVLM(config=config)\n",
    "model.load_pretrained(checkpoint, config, is_eval=evaluate)\n",
    "model = model.to(device)\n",
    "print(\"### Total Params: \", sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "\n",
    "if config['use_roberta']:\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(config['text_encoder'])\n",
    "else:\n",
    "    tokenizer = BertTokenizer.from_pretrained(config['text_encoder'])\n",
    "    \n",
    "    \n",
    "print(\"Creating model_pp\")\n",
    "model_pp = XVLM(config=config_pp)\n",
    "model_pp.load_pretrained(checkpoint_pp, config_pp, is_eval=evaluate)\n",
    "model_pp = model_pp.to(device)\n",
    "print(\"### Total Params: \", sum(p.numel() for p in model_pp.parameters() if p.requires_grad))\n",
    "\n",
    "if config['use_roberta']:\n",
    "    tokenizer_pp = RobertaTokenizer.from_pretrained(config_pp['text_encoder'])\n",
    "else:\n",
    "    tokenizer_pp = BertTokenizer.from_pretrained(config_pp['text_encoder'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d38ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_train_dataset, pp_test_dataset = create_dataset('pascalparts_grounding_bbox', config_pp, evaluate=False)\n",
    "rc_train_dataset, rc_test_dataset = create_dataset('grounding_bbox', config, evaluate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1fb089",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pp_train_dataset), len(pp_test_dataset), len(rc_train_dataset), len(rc_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c44db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Grounding_bbox_PascalParts import val, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5ac6a8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# This one is for the <1% parts taken out\n",
    "\n",
    "# Load Image\n",
    "img_path = '/mv_users/ericwtodd/datasets/PascalPart_People_subset/2008_002485.jpg'\n",
    "# img_path = '/mv_users/ericwtodd/datasets/PascalPart_People_subset/2008_001802.jpg'\n",
    "# img_path = '/mv_users/ericwtodd/datasets/PascalPart_People_subset/2008_001736.jpg'\n",
    "# img_path = '/mv_users/ericwtodd/datasets/PascalPart_People_subset/2008_001808.jpg'\n",
    "image = Image.open(img_path).convert('RGB')\n",
    "image_int = plot_transform(image)\n",
    "image = test_transform(image).unsqueeze(0)\n",
    "\n",
    "\n",
    "# Define Text Prompt\n",
    "prompts = [\"head on the left\", \"head on the right\"]\n",
    "# prompts = [\"right hand of the person on the left\", \"right hand of girl with black hair\"]\n",
    "coords = xvlm_get_bbox(image, prompts, model, tokenizer, config)\n",
    "coords_pp = xvlm_get_bbox(image, prompts, model_pp, tokenizer_pp, config_pp)\n",
    "plot_competing_bbox_prompts(image_int, coords, coords_pp, prompts)\n",
    "\n",
    "prompts2 = [\"face on the left\", \"face on the right\"]\n",
    "coords = xvlm_get_bbox(image, prompts2, model, tokenizer, config)\n",
    "coords_pp = xvlm_get_bbox(image, prompts2, model_pp, tokenizer_pp, config_pp)\n",
    "plot_competing_bbox_prompts(image_int, coords, coords_pp, prompts2)\n",
    "\n",
    "\n",
    "# prompts2 = [\"face of the person in the middle\", \"face of the person on the right\", \"face of the person on the left\"]\n",
    "prompts3 = ['chair', 'person in the middle', \"lamp\"]\n",
    "coords = xvlm_get_bbox(image, prompts3, model, tokenizer, config)\n",
    "coords_pp = xvlm_get_bbox(image, prompts3, model_pp, tokenizer_pp, config_pp)\n",
    "plot_competing_bbox_prompts(image_int, coords, coords_pp, prompts3)\n",
    "\n",
    "prompts4 = [\"lamp\",'lamp on the left', 'picture on the right', \"picture\", \"picture on the left\"]\n",
    "coords = xvlm_get_bbox(image, prompts4, model, tokenizer, config)\n",
    "coords_pp = xvlm_get_bbox(image, prompts4, model_pp, tokenizer_pp, config_pp)\n",
    "plot_competing_bbox_prompts(image_int, coords, coords_pp, prompts4)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Note, while the part model is picking up faces, it has flipped 'left' and 'right'\",\"The part model doesn't do as well if 'head' is the prompt. The object model understands 'head' better than 'face'\", \"The part model still recognizes most concepts, though can be thrown off with left/right. This happens with lamp, but not picture for some reason\", sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fcd6c6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# This one is for the part <0.5% filtered, above is the <1% filtered\n",
    "# Load Image\n",
    "img_path = '/mv_users/ericwtodd/datasets/PascalPart_People_subset/2008_002485.jpg'\n",
    "# img_path = '/mv_users/ericwtodd/datasets/PascalPart_People_subset/2008_001802.jpg'\n",
    "# img_path = '/mv_users/ericwtodd/datasets/PascalPart_People_subset/2008_001736.jpg'\n",
    "# img_path = '/mv_users/ericwtodd/datasets/PascalPart_People_subset/2008_001808.jpg'\n",
    "image = Image.open(img_path).convert('RGB')\n",
    "image_int = plot_transform(image)\n",
    "image = test_transform(image).unsqueeze(0)\n",
    "\n",
    "\n",
    "# Define Text Prompt\n",
    "prompts = [\"head on the left\", \"head on the right\"]\n",
    "# prompts = [\"right hand of the person on the left\", \"right hand of girl with black hair\"]\n",
    "coords = xvlm_get_bbox(image, prompts, model, tokenizer, config)\n",
    "coords_pp = xvlm_get_bbox(image, prompts, model_pp, tokenizer_pp, config_pp)\n",
    "plot_competing_bbox_prompts(image_int, coords, coords_pp, prompts)\n",
    "\n",
    "prompts2 = [\"face on the left\", \"face on the right\"]\n",
    "coords = xvlm_get_bbox(image, prompts2, model, tokenizer, config)\n",
    "coords_pp = xvlm_get_bbox(image, prompts2, model_pp, tokenizer_pp, config_pp)\n",
    "plot_competing_bbox_prompts(image_int, coords, coords_pp, prompts2)\n",
    "\n",
    "\n",
    "# prompts2 = [\"face of the person in the middle\", \"face of the person on the right\", \"face of the person on the left\"]\n",
    "prompts3 = ['chair', 'person in the middle', \"lamp\"]\n",
    "coords = xvlm_get_bbox(image, prompts3, model, tokenizer, config)\n",
    "coords_pp = xvlm_get_bbox(image, prompts3, model_pp, tokenizer_pp, config_pp)\n",
    "plot_competing_bbox_prompts(image_int, coords, coords_pp, prompts3)\n",
    "\n",
    "prompts4 = [\"lamp\",'lamp on the left', 'picture on the right', \"picture\", \"picture on the left\"]\n",
    "coords = xvlm_get_bbox(image, prompts4, model, tokenizer, config)\n",
    "coords_pp = xvlm_get_bbox(image, prompts4, model_pp, tokenizer_pp, config_pp)\n",
    "plot_competing_bbox_prompts(image_int, coords, coords_pp, prompts4)\n",
    "\n",
    "\n",
    "\n",
    "# print(\"Note, while the part model is picking up faces, it has flipped 'left' and 'right'\",\"The part model doesn't do as well if 'head' is the prompt. The object model understands 'head' better than 'face'\", \"The part model still recognizes most concepts, though can be thrown off with left/right. This happens with lamp, but not picture for some reason\", sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec511c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts4 = [\"left hand of girl\", \"hand, not arm, on the chair\"]\n",
    "coords = xvlm_get_bbox(image, prompts4, model, tokenizer, config)\n",
    "coords_pp = xvlm_get_bbox(image, prompts4, model_pp, tokenizer_pp, config_pp)\n",
    "plot_competing_bbox_prompts(image_int, coords, coords_pp, prompts4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8dcfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Image\n",
    "img_path = '/mv_users/ericwtodd/datasets/PascalPart_People_subset/2008_001802.jpg'\n",
    "image = Image.open(img_path).convert('RGB')\n",
    "image_int = plot_transform(image)\n",
    "image = test_transform(image).unsqueeze(0)\n",
    "\n",
    "\n",
    "# Define Text Prompt\n",
    "prompts = [\"left lower leg partially occludes right upper leg\", \"crossed legs of woman on left\", \"left leg on top of right leg\"]\n",
    "coords = xvlm_get_bbox(image, prompts, model, config)\n",
    "plot_bbox_prompts(image_int, coords, prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cb891f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Image\n",
    "img_path = '/mv_users/ericwtodd/datasets/PascalPart_People_subset/2008_001736.jpg'\n",
    "image = Image.open(img_path).convert('RGB')\n",
    "image_int = plot_transform(image)\n",
    "image = test_transform(image).unsqueeze(0)\n",
    "\n",
    "\n",
    "# Define Text Prompt\n",
    "prompts = [\"Left hand\", \"Right hand\", \"head\", \"torso\", \"right leg\", \"left leg\"]\n",
    "\n",
    "coords = xvlm_get_bbox(image, prompts, model, config)\n",
    "plot_bbox_prompts(image_int, coords, prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7870235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Image\n",
    "img_path = '/mv_users/ericwtodd/datasets/PascalPart_People_subset/2008_001808.jpg'\n",
    "image = Image.open(img_path).convert('RGB')\n",
    "image_int = plot_transform(image)\n",
    "image = test_transform(image).unsqueeze(0)\n",
    "\n",
    "\n",
    "# Define Text Prompt\n",
    "# prompts = [\"Left hand\", \"Right hand\", \"head\", \"torso\", \"left leg\", \"right leg\"]\n",
    "prompts = [\"Left hand\", \"Right hand\",]\n",
    "\n",
    "coords = xvlm_get_bbox(image, prompts, model, config)\n",
    "plot_bbox_prompts(image_int, coords, prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e126ac62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Image\n",
    "img_path = '/mv_users/ericwtodd/datasets/PascalPart_People_subset/2008_002801.jpg'\n",
    "image = Image.open(img_path).convert('RGB')\n",
    "image_int = plot_transform(image)\n",
    "image = test_transform(image).unsqueeze(0)\n",
    "\n",
    "\n",
    "# Define Text Prompt\n",
    "prompts = [\"Left hand of girl\", \"hand\", \"person in the middle\"]\n",
    "# prompts = [\"boy with striped sweater\", \"boy with blue turtleneck\",\"winnie the pooh logo\"]\n",
    "\n",
    "coords = xvlm_get_bbox(image, prompts, model, config)\n",
    "plot_bbox_prompts(image_int, coords, prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc56a5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Image\n",
    "img_path = '/mv_users/ericwtodd/datasets/PascalPart_People_subset/2008_002829.jpg'\n",
    "image = Image.open(img_path).convert('RGB')\n",
    "image_int = plot_transform(image)\n",
    "image = test_transform(image).unsqueeze(0)\n",
    "\n",
    "\n",
    "# Define Text Prompt\n",
    "prompts = [\"gray baseball hat\", \"baseball glove\", \"shoes\", \"black pants\", \"fence\", \"silver belt\"]\n",
    "\n",
    "coords = xvlm_get_bbox(image, prompts, model, config)\n",
    "plot_bbox_prompts(image_int, coords, prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9c8534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Image\n",
    "img_path = '/mv_users/ericwtodd/datasets/PascalPart_People_subset/2008_003344.jpg'\n",
    "image = Image.open(img_path).convert('RGB')\n",
    "image_int = plot_transform(image)\n",
    "image = test_transform(image).unsqueeze(0)\n",
    "\n",
    "\n",
    "# Define Text Prompt\n",
    "prompts = [\"person without a hat on\", \"man with a hat on\", \"shoes\", \"right back leg of horse\", \"right front lower leg of horse\", \"right front upper leg of horse\"]\n",
    "# prompts = [\"man without shoes on\", \"man with shoes on\"]\n",
    "# prompts = [\"man not riding a horse\", \"man riding a horse\"]\n",
    "\n",
    "coords = xvlm_get_bbox(image, prompts, model_pp,tokenizer_pp, config_pp)\n",
    "plot_bbox_prompts(image_int, coords, prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363c8e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Image\n",
    "img_path = '/multiview/datasets/mscoco/images/val2017/000000020333.jpg'\n",
    "image = Image.open(img_path).convert('RGB')\n",
    "image_int = plot_transform(image)\n",
    "image = test_transform(image).unsqueeze(0)\n",
    "\n",
    "\n",
    "# Define Text Prompt\n",
    "prompts = [\"left arm\", \"right arm\"]\n",
    "# prompts = [\"right arm\", \"the boy's right arm resting on his knee\", \"right knee\", \"left knee\", \"cat in the background\"]\n",
    "\n",
    "coords = xvlm_get_bbox(image, prompts, model_pp, config_pp)\n",
    "plot_bbox_prompts(image_int, coords, prompts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443a33a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load Image\n",
    "img_path = './images/coco/train2014/COCO_train2014_000000581857.jpg'\n",
    "image = Image.open(img_path).convert('RGB')\n",
    "image_int = plot_transform(image)\n",
    "image = test_transform(image).unsqueeze(0)\n",
    "\n",
    "\n",
    "# Define Text Prompt\n",
    "prompts = [\"all of the green and yellow bananas in the front\", 'white bird cages', \" lady in gray in the back\"]\n",
    "# prompts = [\"right arm\", \"the boy's right arm resting on his knee\", \"right knee\", \"left knee\", \"cat in the background\"]\n",
    "\n",
    "coords = xvlm_get_bbox(image, prompts, model, config)\n",
    "plot_bbox_prompts(image_int, coords, prompts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b56b904",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
