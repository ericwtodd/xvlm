{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaaaa438",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import datetime\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import ruamel.yaml as yaml\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.distributed as dist\n",
    "from torchvision import transforms\n",
    "import torchvision.utils as vutils\n",
    "from PIL import Image\n",
    "from torchvision.io import read_image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import utils\n",
    "from dataset import create_dataset, create_sampler, create_loader\n",
    "from dataset.utils import collect_tensor_result, grounding_eval_bbox, grounding_eval_bbox_vlue\n",
    "from models.model_bbox import XVLM\n",
    "from models.tokenization_bert import BertTokenizer\n",
    "from models.tokenization_roberta import RobertaTokenizer\n",
    "from optim import create_optimizer\n",
    "from refTools.refer_python3 import REFER\n",
    "from scheduler import create_scheduler\n",
    "from utils.hdfs_io import hmkdir, hcopy, hexists\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd46f7ea",
   "metadata": {},
   "source": [
    "## Define BBox Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2356464",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xvlm_get_bbox(image, prompts, model, config):\n",
    "    # Evaluate the model on the image & prompts\n",
    "    results = torch.empty((len(prompts),4))\n",
    "    model.eval()\n",
    "\n",
    "    for i, text in enumerate(prompts):\n",
    "\n",
    "        image = image.to(device)\n",
    "        text_input = tokenizer(text, padding='longest', return_tensors=\"pt\").to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs_coord = model(image, text_input.input_ids, text_input.attention_mask, target_bbox=None)\n",
    "\n",
    "        results[i] = outputs_coord.cpu()\n",
    "\n",
    "    # convert predicted coordinates from normalized center coordinates to xmin, ymin, xmax, ymax\n",
    "    cx, cy, w, h = results[:,0], results[:,1], results[:,2], results[:,3]\n",
    "    coords = torch.stack((cx - w/2, cy-h/2, cx + w/2, cy + h/2)).T * config['image_res']\n",
    "    \n",
    "    return coords\n",
    "    \n",
    "    \n",
    "    \n",
    "def plot_bbox_prompts(image_int, coords, prompts):\n",
    "    # Plot the resulting predictions on top of the original image\n",
    "    plt.figure(figsize=(10,20))\n",
    "    bbox_image = vutils.draw_bounding_boxes(image_int,coords, \n",
    "                                            width=3, \n",
    "                                            labels=[f\"Prompt {i}\" for i in range(len(prompts))],\n",
    "                                            colors=\"red\")\n",
    "\n",
    "    plt.imshow(bbox_image.permute(1,2,0))\n",
    "    plt.xlabel('\\n' + '\\n'.join([f'Prompt {i}: ' + prompts[i] for i in range(len(prompts))]), fontsize=14)\n",
    "    plt.show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c141aea",
   "metadata": {},
   "source": [
    "## Define Parameters & Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd7ea27",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = \"configs/Grounding_bbox.yaml\"\n",
    "config = yaml.load(open(config_file, 'r'), Loader=yaml.Loader)\n",
    "checkpoint = \"model_checkpoints/16m_base_finetune/refcoco_bbox/checkpoint_best.pth\"\n",
    "evaluate = False\n",
    "device = 7\n",
    "\n",
    "\n",
    "# Transformations\n",
    "normalize = transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "        transforms.Resize((config['image_res'], config['image_res']), interpolation=Image.BICUBIC),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ])\n",
    "\n",
    "plot_transform = transforms.Compose([\n",
    "    transforms.Resize((config['image_res'], config['image_res']), interpolation=Image.BICUBIC),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.ConvertImageDtype(torch.uint8),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377b3490",
   "metadata": {},
   "source": [
    "## Load and Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390750ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating model\")\n",
    "model = XVLM(config=config)\n",
    "model.load_pretrained(checkpoint, config, is_eval=evaluate)\n",
    "model = model.to(device)\n",
    "print(\"### Total Params: \", sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "\n",
    "if config['use_roberta']:\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(config['text_encoder'])\n",
    "else:\n",
    "    tokenizer = BertTokenizer.from_pretrained(config['text_encoder'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5ac6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Image\n",
    "img_path = '/mv_users/ericwtodd/datasets/PascalPart_People_subset/2008_002485.jpg'\n",
    "# img_path = '/mv_users/ericwtodd/datasets/PascalPart_People_subset/2008_001802.jpg'\n",
    "# img_path = '/mv_users/ericwtodd/datasets/PascalPart_People_subset/2008_001736.jpg'\n",
    "# img_path = '/mv_users/ericwtodd/datasets/PascalPart_People_subset/2008_001808.jpg'\n",
    "image = Image.open(img_path).convert('RGB')\n",
    "image_int = plot_transform(image)\n",
    "image = test_transform(image).unsqueeze(0)\n",
    "\n",
    "\n",
    "# Define Text Prompt\n",
    "# prompts = [\"The picture on the wall\", \"lamp\", \"door in the background\"]\n",
    "# prompts = [\"face in the middle\", \"face on the right\", \"face on the left\"]\n",
    "prompts = [\"left hand of the person on the left\"]\n",
    "# prompts = [\"left lower leg partially occludes right upper leg\", \"crossed legs\", \"left leg on top of right leg\"]\n",
    "coords = xvlm_get_bbox(image, prompts, model, config)\n",
    "plot_bbox_prompts(image_int, coords, prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8dcfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Image\n",
    "img_path = '/mv_users/ericwtodd/datasets/PascalPart_People_subset/2008_001802.jpg'\n",
    "image = Image.open(img_path).convert('RGB')\n",
    "image_int = plot_transform(image)\n",
    "image = test_transform(image).unsqueeze(0)\n",
    "\n",
    "\n",
    "# Define Text Prompt\n",
    "prompts = [\"left lower leg partially occludes right upper leg\", \"crossed legs of woman on left\", \"left leg on top of right leg\"]\n",
    "coords = xvlm_get_bbox(image, prompts, model, config)\n",
    "plot_bbox_prompts(image_int, coords, prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cb891f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Image\n",
    "img_path = '/mv_users/ericwtodd/datasets/PascalPart_People_subset/2008_001736.jpg'\n",
    "image = Image.open(img_path).convert('RGB')\n",
    "image_int = plot_transform(image)\n",
    "image = test_transform(image).unsqueeze(0)\n",
    "\n",
    "\n",
    "# Define Text Prompt\n",
    "prompts = [\"Left hand\", \"Right hand\", \"head\", \"torso\", \"right leg\", \"left leg\"]\n",
    "\n",
    "coords = xvlm_get_bbox(image, prompts, model, config)\n",
    "plot_bbox_prompts(image_int, coords, prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7870235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Image\n",
    "img_path = '/mv_users/ericwtodd/datasets/PascalPart_People_subset/2008_001808.jpg'\n",
    "image = Image.open(img_path).convert('RGB')\n",
    "image_int = plot_transform(image)\n",
    "image = test_transform(image).unsqueeze(0)\n",
    "\n",
    "\n",
    "# Define Text Prompt\n",
    "# prompts = [\"Left hand\", \"Right hand\", \"head\", \"torso\", \"left leg\", \"right leg\"]\n",
    "prompts = [\"Left hand\", \"Right hand\",]\n",
    "\n",
    "coords = xvlm_get_bbox(image, prompts, model, config)\n",
    "plot_bbox_prompts(image_int, coords, prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e126ac62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Image\n",
    "img_path = '/mv_users/ericwtodd/datasets/PascalPart_People_subset/2008_002801.jpg'\n",
    "image = Image.open(img_path).convert('RGB')\n",
    "image_int = plot_transform(image)\n",
    "image = test_transform(image).unsqueeze(0)\n",
    "\n",
    "\n",
    "# Define Text Prompt\n",
    "# prompts = [\"Left hand of girl\", \"left hand of boy\", \"hand\", \"person in the middle\"]\n",
    "prompts = [\"boy with striped sweater\", \"boy with blue turtleneck\",\"winnie the pooh logo\"]\n",
    "\n",
    "coords = xvlm_get_bbox(image, prompts, model, config)\n",
    "plot_bbox_prompts(image_int, coords, prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc56a5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Image\n",
    "img_path = '/mv_users/ericwtodd/datasets/PascalPart_People_subset/2008_002829.jpg'\n",
    "image = Image.open(img_path).convert('RGB')\n",
    "image_int = plot_transform(image)\n",
    "image = test_transform(image).unsqueeze(0)\n",
    "\n",
    "\n",
    "# Define Text Prompt\n",
    "prompts = [\"gray baseball hat\", \"baseball glove\", \"shoes\", \"black pants\", \"fence\", \"silver belt\"]\n",
    "\n",
    "coords = xvlm_get_bbox(image, prompts, model, config)\n",
    "plot_bbox_prompts(image_int, coords, prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9c8534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Image\n",
    "img_path = '/mv_users/ericwtodd/datasets/PascalPart_People_subset/2008_003344.jpg'\n",
    "image = Image.open(img_path).convert('RGB')\n",
    "image_int = plot_transform(image)\n",
    "image = test_transform(image).unsqueeze(0)\n",
    "\n",
    "\n",
    "# Define Text Prompt\n",
    "prompts = [\"man without a hat on\", \"man with a hat on\", \"shoes\", \"bare feet of human\"]\n",
    "# prompts = [\"man without shoes on\", \"man with shoes on\"]\n",
    "# prompts = [\"man not riding a horse\", \"man riding a horse\"]\n",
    "\n",
    "coords = xvlm_get_bbox(image, prompts, model, config)\n",
    "plot_bbox_prompts(image_int, coords, prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363c8e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Image\n",
    "img_path = '/multiview/datasets/mscoco/images/val2017/000000020333.jpg'\n",
    "image = Image.open(img_path).convert('RGB')\n",
    "image_int = plot_transform(image)\n",
    "image = test_transform(image).unsqueeze(0)\n",
    "\n",
    "\n",
    "# Define Text Prompt\n",
    "prompts = [\"left arm\", \"right arm\"]\n",
    "# prompts = [\"right arm\", \"the boy's right arm resting on his knee\", \"right knee\", \"left knee\", \"cat in the background\"]\n",
    "\n",
    "coords = xvlm_get_bbox(image, prompts, model, config)\n",
    "plot_bbox_prompts(image_int, coords, prompts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443a33a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load Image\n",
    "img_path = './images/coco/train2014/COCO_train2014_000000581857.jpg'\n",
    "image = Image.open(img_path).convert('RGB')\n",
    "image_int = plot_transform(image)\n",
    "image = test_transform(image).unsqueeze(0)\n",
    "\n",
    "\n",
    "# Define Text Prompt\n",
    "prompts = [\"all of the green and yellow bananas in the front\", 'white bird cages', \" lady in gray in the back\"]\n",
    "# prompts = [\"right arm\", \"the boy's right arm resting on his knee\", \"right knee\", \"left knee\", \"cat in the background\"]\n",
    "\n",
    "coords = xvlm_get_bbox(image, prompts, model, config)\n",
    "plot_bbox_prompts(image_int, coords, prompts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b56b904",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
